# Streaming in LangGraph

* **What is Streaming?**

  * **Streaming** in LLMs refers to the process where the model **starts sending tokens (words) as soon as they are generated**, rather than waiting for the entire response to complete.
  * **Why Streaming is Useful**

    1. **Faster Response Time** – Reduces latency and user drop-off.
    2. **Human-like Conversation** – Feels more alive, builds trust, and keeps users engaged.
    3. **Multi-modal UIs** – Supports real-time updates for complex interfaces.
    4. **Better UX for Long Outputs** – Useful for code generation, long text, or multi-step responses.
    5. **Cancelable Midway** – Saves tokens and computational resources if the user cancels.
    6. **Interleaved UI Updates** – Allows showing intermediate statuses like "thinking…" or tool results while the response is generated.

---

* **How to Enable Streaming in LangGraph**

  * In LangGraph, instead of using `.invoke()`, you can use `.stream()`, which **returns a generator** that yields tokens as they are produced.
  * This enables real-time token-level updates for the UI or further processing.

---

* **What Are Generators?**

  * A **generator** is a special type of **iterator in Python** that produces values one at a time using the `yield` statement.
  * Instead of storing all values in memory, generators **pause their state** between yields and produce values on-the-fly.
  * **Benefits**

    * Memory-efficient for large datasets or continuous streams.
    * Supports incremental processing of data, ideal for streaming LLM responses.

---

* **Streaming Example in LangGraph**

```python
from dataclasses import dataclass
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START

@dataclass
class MyState:
    topic: str
    joke: str = ""

model = init_chat_model(model="gpt-4o-mini")

def call_model(state: MyState):
    """Call the LLM to generate a joke about a topic"""
    model_response = model.invoke(
        [{"role": "user", "content": f"Generate a joke about {state.topic}"}]
    )
    return {"joke": model_response.content}

graph = (
    StateGraph(MyState)
    .add_node(call_model)
    .add_edge(START, "call_model")
    .compile()
)

# Streaming messages from the graph
for message_chunk, metadata in graph.stream(
    {"topic": "ice cream"},
    stream_mode="messages",
):
    if message_chunk.content:
        print(message_chunk.content, end="|", flush=True)
```

* **Important Notes on Streaming Output**

  * In `stream_mode="messages"`, each iteration returns a **tuple `(message_chunk, metadata)`**:

    * `message_chunk`: The token or message segment generated by the LLM.
    * `metadata`: A dictionary containing details about the **graph node** and **LLM invocation context**.

---

* **References**

  * Official Documentation: [LangGraph Streaming](https://docs.langchain.com/oss/python/langgraph/streaming)

---

